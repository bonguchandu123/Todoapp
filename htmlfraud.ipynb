{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bonguchandu123/Todoapp/blob/main/htmlfraud.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tENKxXcQAOUy",
        "outputId": "98d960cc-1018-41ab-c40f-ed7cc56f5eb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (0.116.1)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (0.35.0)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.3.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Collecting pennylane\n",
            "  Downloading pennylane-0.42.3-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.47.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from fastapi) (2.11.7)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (4.14.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from pennylane) (3.5)\n",
            "Collecting rustworkx>=0.14.0 (from pennylane)\n",
            "  Downloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.8.0)\n",
            "Collecting appdirs (from pennylane)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting autoray<0.8,>=0.6.11 (from pennylane)\n",
            "  Downloading autoray-0.7.2-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from pennylane) (5.5.2)\n",
            "Collecting pennylane-lightning>=0.42 (from pennylane)\n",
            "  Downloading pennylane_lightning-0.42.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.32.4)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.13.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from pennylane) (25.0)\n",
            "Collecting diastatic-malt (from pennylane)\n",
            "  Downloading diastatic_malt-2.15.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Collecting scipy-openblas32>=0.3.26 (from pennylane-lightning>=0.42->pennylane)\n",
            "  Downloading scipy_openblas32-0.3.30.0.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.48.0,>=0.40.0->fastapi) (4.10.0)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (1.6.3)\n",
            "Requirement already satisfied: gast in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (0.6.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (3.1.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (2025.8.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse->diastatic-malt->pennylane) (0.45.1)\n",
            "Downloading pyngrok-7.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading pennylane-0.42.3-py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autoray-0.7.2-py3-none-any.whl (930 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m930.8/930.8 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pennylane_lightning-0.42.0-cp312-cp312-manylinux_2_28_x86_64.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading diastatic_malt-2.15.2-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy_openblas32-0.3.30.0.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: appdirs, scipy-openblas32, rustworkx, pyngrok, autoray, diastatic-malt, pennylane-lightning, pennylane\n",
            "Successfully installed appdirs-1.4.4 autoray-0.7.2 diastatic-malt-2.15.2 pennylane-0.42.3 pennylane-lightning-0.42.0 pyngrok-7.3.0 rustworkx-0.17.1 scipy-openblas32-0.3.30.0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi uvicorn pyngrok scikit-learn pandas pennylane matplotlib\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "EWiaQHQYBHnY",
        "outputId": "03347f82-8e07-44b5-84df-21e3c3433e32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (0.35.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (0.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install uvicorn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjLYtKOGEJWc",
        "outputId": "e98e700d-d2d2-4586-a496-b0f8a7256e76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bson\n",
            "  Downloading bson-0.5.10.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from bson) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from bson) (1.17.0)\n",
            "Building wheels for collected packages: bson\n",
            "  Building wheel for bson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bson: filename=bson-0.5.10-py3-none-any.whl size=11974 sha256=4c8efc5db137080e3ccdc54122d6877c55e59e649274461abbe92fc201d020ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/08/cb/40b0d49a41c8a6554999e3a07f61944be9aa7500d3a64faf61\n",
            "Successfully built bson\n",
            "Installing collected packages: bson\n",
            "Successfully installed bson-0.5.10\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.12/dist-packages (2.10.1)\n",
            "Collecting bcrypt\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Collecting motor\n",
            "  Downloading motor-3.7.1-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting pymongo\n",
            "  Downloading pymongo-4.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading motor-3.7.1-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymongo-4.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dnspython, bcrypt, pymongo, motor\n",
            "Successfully installed bcrypt-4.3.0 dnspython-2.7.0 motor-3.7.1 pymongo-4.14.1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install bson\n",
        "\n",
        "!pip install PyJWT bcrypt motor pymongo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhzSp9eOrxDh",
        "outputId": "247bc09d-b4e1-482c-c2eb-356c55d95285"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting python-jose[cryptography]\n",
            "  Downloading python_jose-3.5.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting ecdsa!=0.15 (from python-jose[cryptography])\n",
            "  Downloading ecdsa-0.19.1-py2.py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: rsa!=4.1.1,!=4.4,<5.0,>=4.0 in /usr/local/lib/python3.12/dist-packages (from python-jose[cryptography]) (4.9.1)\n",
            "Requirement already satisfied: pyasn1>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from python-jose[cryptography]) (0.6.1)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.12/dist-packages (from python-jose[cryptography]) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=3.4.0->python-jose[cryptography]) (1.17.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from ecdsa!=0.15->python-jose[cryptography]) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=3.4.0->python-jose[cryptography]) (2.22)\n",
            "Downloading ecdsa-0.19.1-py2.py3-none-any.whl (150 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/150.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.6/150.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_jose-3.5.0-py2.py3-none-any.whl (34 kB)\n",
            "Installing collected packages: ecdsa, python-jose\n",
            "Successfully installed ecdsa-0.19.1 python-jose-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install python-jose[cryptography]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sednmomHr_Ws",
        "outputId": "3eff3ad3-68a8-4911-c5e5-8e94e8c2b2a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting passlib[bcrypt]\n",
            "  Downloading passlib-1.7.4-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: bcrypt>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from passlib[bcrypt]) (4.3.0)\n",
            "Downloading passlib-1.7.4-py2.py3-none-any.whl (525 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/525.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/525.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.6/525.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: passlib\n",
            "Successfully installed passlib-1.7.4\n"
          ]
        }
      ],
      "source": [
        "!pip install passlib[bcrypt]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9Evp9PfDlQm",
        "outputId": "f409ccce-9729-4a2a-a98c-ac6a6839f2b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok config add-authtoken 31e3W5Dm2p41ltYXjyGN5qJMUOc_3Pj1mN7dntp2JXHNceREo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBfPEhT9OSHB",
        "outputId": "e01b5ce4-6e5a-4c2e-f499-1181fb1d7f2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 FastAPI is live at: NgrokTunnel: \"https://b699072bd068.ngrok-free.app\" -> \"http://localhost:8000\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [1618]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Starting Fraud Detection API...\n",
            "INFO:     2409:40f0:410c:41ea:35e7:542b:693a:953d:0 - \"OPTIONS /model-status HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:35e7:542b:693a:953d:0 - \"OPTIONS /model-status HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:35e7:542b:693a:953d:0 - \"GET /model-status HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:35e7:542b:693a:953d:0 - \"GET /model-status HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:35e7:542b:693a:953d:0 - \"OPTIONS /health HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:35e7:542b:693a:953d:0 - \"GET /health HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:35e7:542b:693a:953d:0 - \"OPTIONS /saved-predictions HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:35e7:542b:693a:953d:0 - \"OPTIONS /saved-predictions HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:35e7:542b:693a:953d:0 - \"OPTIONS /user-stats HTTP/1.1\" 200 OK\n",
            "DEBUG: credentials received: scheme='Bearer' credentials='eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJjaGFuZHUxMiIsImV4cCI6MTc1NjI3MjU5M30.ougBGZaRR-CIKLxptqowgi0FpdLEDqoLttKHC2-YzgU'\n",
            "DEBUG: Raw token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJja...\n",
            "DEBUG: Processing token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJja...\n",
            "DEBUG: Token has expired\n",
            "INFO:     2409:40f0:410c:41ea:35e7:542b:693a:953d:0 - \"GET /saved-predictions HTTP/1.1\" 401 Unauthorized\n",
            "INFO:     2409:40f0:410c:41ea:35e7:542b:693a:953d:0 - \"OPTIONS /user-stats HTTP/1.1\" 200 OK\n",
            "DEBUG: credentials received: scheme='Bearer' credentials='eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJjaGFuZHUxMiIsImV4cCI6MTc1NjI3MjU5M30.ougBGZaRR-CIKLxptqowgi0FpdLEDqoLttKHC2-YzgU'\n",
            "DEBUG: Raw token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJja...\n",
            "DEBUG: Processing token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJja...\n",
            "DEBUG: Token has expired\n",
            "INFO:     2409:40f0:410c:41ea:35e7:542b:693a:953d:0 - \"GET /saved-predictions HTTP/1.1\" 401 Unauthorized\n",
            "DEBUG: credentials received: scheme='Bearer' credentials='eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJjaGFuZHUxMiIsImV4cCI6MTc1NjI3MjU5M30.ougBGZaRR-CIKLxptqowgi0FpdLEDqoLttKHC2-YzgU'\n",
            "DEBUG: Raw token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJja...\n",
            "DEBUG: Processing token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJja...\n",
            "DEBUG: Token has expired\n",
            "INFO:     2409:40f0:410c:41ea:35e7:542b:693a:953d:0 - \"GET /user-stats HTTP/1.1\" 401 Unauthorized\n",
            "DEBUG: credentials received: scheme='Bearer' credentials='eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJjaGFuZHUxMiIsImV4cCI6MTc1NjI3MjU5M30.ougBGZaRR-CIKLxptqowgi0FpdLEDqoLttKHC2-YzgU'\n",
            "DEBUG: Raw token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJja...\n",
            "DEBUG: Processing token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJja...\n",
            "DEBUG: Token has expired\n",
            "INFO:     2409:40f0:410c:41ea:35e7:542b:693a:953d:0 - \"GET /saved-predictions HTTP/1.1\" 401 Unauthorized\n",
            "DEBUG: credentials received: scheme='Bearer' credentials='eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJjaGFuZHUxMiIsImV4cCI6MTc1NjI3MjU5M30.ougBGZaRR-CIKLxptqowgi0FpdLEDqoLttKHC2-YzgU'\n",
            "DEBUG: Raw token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJja...\n",
            "DEBUG: Processing token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJja...\n",
            "DEBUG: Token has expired\n",
            "INFO:     2409:40f0:410c:41ea:35e7:542b:693a:953d:0 - \"GET /user-stats HTTP/1.1\" 401 Unauthorized\n",
            "DEBUG: credentials received: scheme='Bearer' credentials='eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJjaGFuZHUxMiIsImV4cCI6MTc1NjI3MjU5M30.ougBGZaRR-CIKLxptqowgi0FpdLEDqoLttKHC2-YzgU'\n",
            "DEBUG: Raw token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJja...\n",
            "DEBUG: Processing token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJja...\n",
            "DEBUG: Token has expired\n",
            "INFO:     2409:40f0:410c:41ea:35e7:542b:693a:953d:0 - \"GET /saved-predictions HTTP/1.1\" 401 Unauthorized\n",
            "INFO:     2409:40f0:410c:41ea:35e7:542b:693a:953d:0 - \"OPTIONS /login HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:35e7:542b:693a:953d:0 - \"POST /login HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-4040422513.py:131: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  expire = datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     2409:40f0:410c:41ea:35e7:542b:693a:953d:0 - \"GET /model-status HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:35e7:542b:693a:953d:0 - \"GET /model-status HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:35e7:542b:693a:953d:0 - \"GET /health HTTP/1.1\" 200 OK\n",
            "DEBUG: credentials received: scheme='Bearer' credentials='eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJjaGFuZHUxMiIsImV4cCI6MTc1NjI4MDkzOH0.SHJ0U7wL8-j60E5Ci5uzZh2PjGl75SNJmRUKFDhrdzU'\n",
            "DEBUG: Raw token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJja...\n",
            "DEBUG: Processing token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJja...\n",
            "DEBUG: JWT payload: {'sub': 'chandu12', 'exp': 1756280938}\n",
            "DEBUG: Token verified successfully for user: chandu12\n",
            "DEBUG: credentials received: scheme='Bearer' credentials='eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJjaGFuZHUxMiIsImV4cCI6MTc1NjI4MDkzOH0.SHJ0U7wL8-j60E5Ci5uzZh2PjGl75SNJmRUKFDhrdzU'\n",
            "DEBUG: Raw token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJja...\n",
            "DEBUG: Processing token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJja...\n",
            "DEBUG: JWT payload: {'sub': 'chandu12', 'exp': 1756280938}\n",
            "DEBUG: Token verified successfully for user: chandu12\n",
            "INFO:     2409:40f0:410c:41ea:35e7:542b:693a:953d:0 - \"GET /saved-predictions HTTP/1.1\" 200 OK\n",
            "DEBUG: credentials received: scheme='Bearer' credentials='eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJjaGFuZHUxMiIsImV4cCI6MTc1NjI4MDkzOH0.SHJ0U7wL8-j60E5Ci5uzZh2PjGl75SNJmRUKFDhrdzU'\n",
            "DEBUG: Raw token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJja...\n",
            "DEBUG: Processing token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJja...\n",
            "DEBUG: JWT payload: {'sub': 'chandu12', 'exp': 1756280938}\n",
            "DEBUG: Token verified successfully for user: chandu12\n",
            "INFO:     2409:40f0:410c:41ea:35e7:542b:693a:953d:0 - \"GET /saved-predictions HTTP/1.1\" 200 OK\n",
            "DEBUG: credentials received: scheme='Bearer' credentials='eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJjaGFuZHUxMiIsImV4cCI6MTc1NjI4MDkzOH0.SHJ0U7wL8-j60E5Ci5uzZh2PjGl75SNJmRUKFDhrdzU'\n",
            "DEBUG: Raw token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJja...\n",
            "DEBUG: Processing token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJja...\n",
            "DEBUG: JWT payload: {'sub': 'chandu12', 'exp': 1756280938}\n",
            "DEBUG: Token verified successfully for user: chandu12\n",
            "INFO:     2409:40f0:410c:41ea:35e7:542b:693a:953d:0 - \"GET /user-stats HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:35e7:542b:693a:953d:0 - \"GET /saved-predictions HTTP/1.1\" 200 OK\n",
            "DEBUG: credentials received: scheme='Bearer' credentials='eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJjaGFuZHUxMiIsImV4cCI6MTc1NjI4MDkzOH0.SHJ0U7wL8-j60E5Ci5uzZh2PjGl75SNJmRUKFDhrdzU'\n",
            "DEBUG: Raw token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJja...\n",
            "DEBUG: Processing token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJja...\n",
            "DEBUG: JWT payload: {'sub': 'chandu12', 'exp': 1756280938}\n",
            "DEBUG: Token verified successfully for user: chandu12\n",
            "DEBUG: credentials received: scheme='Bearer' credentials='eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJjaGFuZHUxMiIsImV4cCI6MTc1NjI4MDkzOH0.SHJ0U7wL8-j60E5Ci5uzZh2PjGl75SNJmRUKFDhrdzU'\n",
            "DEBUG: Raw token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJja...\n",
            "DEBUG: Processing token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJja...\n",
            "DEBUG: JWT payload: {'sub': 'chandu12', 'exp': 1756280938}\n",
            "DEBUG: Token verified successfully for user: chandu12\n",
            "INFO:     2409:40f0:410c:41ea:35e7:542b:693a:953d:0 - \"GET /saved-predictions HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:35e7:542b:693a:953d:0 - \"GET /user-stats HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:35e7:542b:693a:953d:0 - \"GET /health HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:35e7:542b:693a:953d:0 - \"GET /health HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:35e7:542b:693a:953d:0 - \"GET /model-status HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40f0:410c:41ea:35e7:542b:693a:953d:0 - \"GET /health HTTP/1.1\" 200 OK\n"
          ]
        }
      ],
      "source": [
        "from fastapi import FastAPI, File, UploadFile, HTTPException, Depends, status\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\n",
        "from pydantic import BaseModel\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import io\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, f1_score, classification_report, confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as qnp\n",
        "import pickle\n",
        "import json\n",
        "from typing import List, Optional, Dict, Any\n",
        "import time\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "from sklearn.metrics import roc_auc_score, f1_score, precision_recall_fscore_support, classification_report\n",
        "import jwt\n",
        "from datetime import datetime, timedelta\n",
        "import bcrypt\n",
        "from motor.motor_asyncio import AsyncIOMotorClient\n",
        "import os\n",
        "from bson import ObjectId\n",
        "from contextlib import asynccontextmanager\n",
        "\n",
        "\n",
        "MONGODB_URL = \"mongodb+srv://bonguchandu:Chandu123456@cluster0.su81k.mongodb.net\"\n",
        "client = AsyncIOMotorClient(MONGODB_URL)\n",
        "db = client.fraud_detection\n",
        "\n",
        "\n",
        "SECRET_KEY = \"your-secret-key-change-in-production\"\n",
        "ALGORITHM = \"HS256\"\n",
        "ACCESS_TOKEN_EXPIRE_MINUTES = 30\n",
        "\n",
        "\n",
        "security = HTTPBearer(auto_error=False)\n",
        "\n",
        "@asynccontextmanager\n",
        "async def lifespan(app: FastAPI):\n",
        "\n",
        "    print(\"🚀 Starting Fraud Detection API...\")\n",
        "    yield\n",
        "\n",
        "    await client.close()\n",
        "    print(\"👋 Shutting down API...\")\n",
        "\n",
        "app = FastAPI(\n",
        "    title=\"Quantum Fraud Detection API\",\n",
        "    version=\"2.0.0\",\n",
        "    lifespan=lifespan\n",
        ")\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "\n",
        "quantum_model = None\n",
        "scaler_quantum = None\n",
        "classical_models = {}\n",
        "scaler_classical = None\n",
        "\n",
        "creditcard_quantum_model = None\n",
        "creditcard_scaler_quantum = None\n",
        "creditcard_classical_models = {}\n",
        "creditcard_scaler_classical = None\n",
        "creditcard_data = None\n",
        "\n",
        "\n",
        "n_qubits = 4\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "def feature_map(x):\n",
        "    for i in range(n_qubits):\n",
        "        qml.RY(x[i], wires=i)\n",
        "    for i in range(n_qubits-1):\n",
        "        qml.CNOT(wires=[i, i+1])\n",
        "\n",
        "def ansatz(weights):\n",
        "    for i in range(n_qubits):\n",
        "        qml.RY(weights[i], wires=i)\n",
        "        qml.RZ(weights[i+n_qubits], wires=i)\n",
        "\n",
        "@qml.qnode(dev, interface=\"autograd\")\n",
        "def circuit(x, weights):\n",
        "    feature_map(x)\n",
        "    ansatz(weights)\n",
        "    return qml.expval(qml.PauliZ(0))\n",
        "\n",
        "def predict_batch(W, Xb):\n",
        "    outs = []\n",
        "    for x in Xb:\n",
        "        z = circuit(x, W)\n",
        "        p = 0.5 * (1 + z)\n",
        "        outs.append(qnp.clip(p, 1e-7, 1-1e-7))\n",
        "    return qnp.stack(outs)\n",
        "\n",
        "def bce_loss(W, Xb, yb):\n",
        "    p = predict_batch(W, Xb)\n",
        "    return -qnp.mean(yb*qnp.log(p) + (1-yb)*qnp.log(1-p))\n",
        "def make_toy_fraud(n=1200, seed=42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n0 = int(n*0.9)\n",
        "\n",
        "    X0 = rng.normal(loc=[0.3,0.4,0.4,0.3], scale=[0.2,0.2,0.2,0.2], size=(n0,4))\n",
        "    y0 = np.zeros(n0)\n",
        "    n1 = n - n0\n",
        "    X1 = rng.normal(loc=[0.6,0.7,0.7,0.6], scale=[0.2,0.2,0.2,0.2], size=(n1,4))\n",
        "    y1 = np.ones(n1)\n",
        "    X = np.vstack([X0, X1])\n",
        "    y = np.concatenate([y0, y1])\n",
        "    X = np.clip(X, 0, 1)\n",
        "    return X, y.astype(int)\n",
        "\n",
        "\n",
        "def create_access_token(data: dict):\n",
        "    to_encode = data.copy()\n",
        "    expire = datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)\n",
        "    to_encode.update({\"exp\": expire})\n",
        "    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)\n",
        "    return encoded_jwt\n",
        "\n",
        "def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):\n",
        "    print(f\"DEBUG: credentials received: {credentials}\")\n",
        "\n",
        "    if not credentials:\n",
        "        print(\"DEBUG: No credentials provided\")\n",
        "        raise HTTPException(\n",
        "            status_code=status.HTTP_401_UNAUTHORIZED,\n",
        "            detail=\"Authorization header missing\",\n",
        "            headers={\"WWW-Authenticate\": \"Bearer\"},\n",
        "        )\n",
        "\n",
        "    try:\n",
        "\n",
        "        token = credentials.credentials\n",
        "        print(f\"DEBUG: Raw token: {token[:50]}...\" if len(token) > 50 else f\"DEBUG: Raw token: {token}\")\n",
        "\n",
        "\n",
        "        if token.startswith(\"Bearer \"):\n",
        "            token = token[7:]\n",
        "            print(\"DEBUG: Removed Bearer prefix\")\n",
        "\n",
        "        print(f\"DEBUG: Processing token: {token[:50]}...\" if len(token) > 50 else f\"DEBUG: Processing token: {token}\")\n",
        "\n",
        "\n",
        "        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])\n",
        "        print(f\"DEBUG: JWT payload: {payload}\")\n",
        "\n",
        "        username: str = payload.get(\"sub\")\n",
        "        if username is None:\n",
        "            print(\"DEBUG: No username in token payload\")\n",
        "            raise HTTPException(\n",
        "                status_code=status.HTTP_401_UNAUTHORIZED,\n",
        "                detail=\"Invalid token: no username found\",\n",
        "                headers={\"WWW-Authenticate\": \"Bearer\"},\n",
        "            )\n",
        "\n",
        "        print(f\"DEBUG: Token verified successfully for user: {username}\")\n",
        "        return username\n",
        "\n",
        "    except jwt.ExpiredSignatureError:\n",
        "        print(\"DEBUG: Token has expired\")\n",
        "        raise HTTPException(\n",
        "            status_code=status.HTTP_401_UNAUTHORIZED,\n",
        "            detail=\"Token has expired\",\n",
        "            headers={\"WWW-Authenticate\": \"Bearer\"},\n",
        "        )\n",
        "    except jwt.InvalidTokenError as e:\n",
        "        print(f\"DEBUG: Invalid token error: {str(e)}\")\n",
        "        raise HTTPException(\n",
        "            status_code=status.HTTP_401_UNAUTHORIZED,\n",
        "            detail=f\"Invalid token: {str(e)}\",\n",
        "            headers={\"WWW-Authenticate\": \"Bearer\"},\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"DEBUG: Unexpected error in token verification: {str(e)}\")\n",
        "        raise HTTPException(\n",
        "            status_code=status.HTTP_401_UNAUTHORIZED,\n",
        "            detail=f\"Token verification failed: {str(e)}\",\n",
        "            headers={\"WWW-Authenticate\": \"Bearer\"},\n",
        "        )\n",
        "\n",
        "\n",
        "class UserRegister(BaseModel):\n",
        "    username: str\n",
        "    email: str\n",
        "    password: str\n",
        "\n",
        "class UserLogin(BaseModel):\n",
        "    username: str\n",
        "    password: str\n",
        "class PasswordGenerateRequest(BaseModel):\n",
        "    length: int = 12\n",
        "    include_special: bool = True\n",
        "\n",
        "class TransactionInput(BaseModel):\n",
        "    amount: float\n",
        "    hour: int\n",
        "    device: str\n",
        "    merchant_risk: float\n",
        "    merchant_category: str\n",
        "    transaction_type: str\n",
        "    cardholder_age: int\n",
        "\n",
        "class CreditCardInput(BaseModel):\n",
        "    V1: float\n",
        "    V2: float\n",
        "    V3: float\n",
        "    V4: float\n",
        "    V5: float\n",
        "    V6: float\n",
        "    V7: float\n",
        "    V8: float\n",
        "    V9: float\n",
        "    V10: float\n",
        "    V11: float\n",
        "    V12: float\n",
        "    V13: float\n",
        "    V14: float\n",
        "    V15: float\n",
        "    V16: float\n",
        "    V17: float\n",
        "    V18: float\n",
        "    V19: float\n",
        "    V20: float\n",
        "    V21: float\n",
        "    V22: float\n",
        "    V23: float\n",
        "    V24: float\n",
        "    V25: float\n",
        "    V26: float\n",
        "    V27: float\n",
        "    V28: float\n",
        "    Time: float\n",
        "    Amount: float\n",
        "\n",
        "class TrainingConfig(BaseModel):\n",
        "    epochs: int = 20\n",
        "    batch_size: int = 64\n",
        "    stepsize: float = 0.2\n",
        "    seed: int = 123\n",
        "\n",
        "class PredictionResponse(BaseModel):\n",
        "    quantum_prediction: float\n",
        "    classical_rf_prediction: float\n",
        "    classical_lr_prediction: float\n",
        "    hybrid_prediction: float\n",
        "    is_fraud: bool\n",
        "\n",
        "class CreditCardPredictionResponse(BaseModel):\n",
        "    quantum_prediction: float\n",
        "    classical_rf_prediction: float\n",
        "    classical_lr_prediction: float\n",
        "    hybrid_prediction: float\n",
        "    is_fraud: bool\n",
        "    confidence: float\n",
        "\n",
        "class TrainingResponse(BaseModel):\n",
        "    success: bool\n",
        "    message: str\n",
        "    metrics: dict\n",
        "\n",
        "class AnalyticsResponse(BaseModel):\n",
        "    model_performance: dict\n",
        "    feature_importance: dict\n",
        "    confusion_matrix: List[List[int]]\n",
        "\n",
        "class SavedPrediction(BaseModel):\n",
        "    prediction_type: str\n",
        "    input_data: dict\n",
        "    result: dict\n",
        "    timestamp: datetime\n",
        "\n",
        "class SavePredictionRequest(BaseModel):\n",
        "    prediction_type: str\n",
        "    input_data: dict\n",
        "    result: dict\n",
        "\n",
        "\n",
        "@app.post(\"/register\")\n",
        "async def register_user(user: UserRegister):\n",
        "    try:\n",
        "\n",
        "        existing_user = await db.users.find_one({\"$or\": [{\"username\": user.username}, {\"email\": user.email}]})\n",
        "        if existing_user:\n",
        "            raise HTTPException(status_code=400, detail=\"Username or email already exists\")\n",
        "\n",
        "        # Hash password\n",
        "        hashed_password = bcrypt.hashpw(user.password.encode('utf-8'), bcrypt.gensalt())\n",
        "\n",
        "        # Insert user\n",
        "        user_doc = {\n",
        "            \"username\": user.username,\n",
        "            \"email\": user.email,\n",
        "            \"password\": hashed_password,\n",
        "            \"created_at\": datetime.utcnow()\n",
        "        }\n",
        "\n",
        "        result = await db.users.insert_one(user_doc)\n",
        "\n",
        "        return {\n",
        "            \"success\": True,\n",
        "            \"message\": \"User registered successfully\",\n",
        "            \"user_id\": str(result.inserted_id)\n",
        "        }\n",
        "    except HTTPException:\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.post(\"/login\")\n",
        "async def login_user(user: UserLogin):\n",
        "    try:\n",
        "\n",
        "        db_user = await db.users.find_one({\"username\": user.username})\n",
        "        if not db_user:\n",
        "            raise HTTPException(status_code=401, detail=\"Invalid username or password\")\n",
        "\n",
        "\n",
        "        if not bcrypt.checkpw(user.password.encode('utf-8'), db_user[\"password\"]):\n",
        "            raise HTTPException(status_code=401, detail=\"Invalid username or password\")\n",
        "\n",
        "\n",
        "        access_token = create_access_token(data={\"sub\": user.username})\n",
        "\n",
        "        return {\n",
        "            \"access_token\": access_token,\n",
        "            \"token_type\": \"bearer\",\n",
        "            \"username\": user.username\n",
        "        }\n",
        "    except HTTPException:\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "\n",
        "def generate_quantum_password(length=12, include_special=True):\n",
        "    \"\"\"Generate a quantum-inspired password using quantum random number generation\"\"\"\n",
        "    import string\n",
        "    import numpy as np\n",
        "\n",
        "    try:\n",
        "\n",
        "        rng_dev = qml.device(\"default.qubit\", wires=1, shots=100)\n",
        "\n",
        "        @qml.qnode(rng_dev)\n",
        "        def quantum_rng():\n",
        "            qml.Hadamard(wires=0)\n",
        "            return qml.sample(qml.PauliZ(0))\n",
        "\n",
        "\n",
        "        quantum_bits = []\n",
        "        for _ in range(length * 4):\n",
        "            samples = quantum_rng()\n",
        "\n",
        "            bit = 1 if np.mean(samples) > 0 else 0\n",
        "            quantum_bits.append(bit)\n",
        "\n",
        "        lowercase = string.ascii_lowercase\n",
        "        uppercase = string.ascii_uppercase\n",
        "        digits = string.digits\n",
        "        special = \"!@#$%&*\" if include_special else \"\"\n",
        "        all_chars = lowercase + uppercase + digits + special\n",
        "\n",
        "\n",
        "        password = []\n",
        "\n",
        "\n",
        "        if length >= 4:\n",
        "\n",
        "            password.append(lowercase[sum(quantum_bits[0:4]) % len(lowercase)])\n",
        "            password.append(uppercase[sum(quantum_bits[4:8]) % len(uppercase)])\n",
        "            password.append(digits[sum(quantum_bits[8:12]) % len(digits)])\n",
        "            if include_special and special:\n",
        "                password.append(special[sum(quantum_bits[12:16]) % len(special)])\n",
        "\n",
        "\n",
        "        start_idx = len(password)\n",
        "        for i in range(start_idx, length):\n",
        "            bit_group = quantum_bits[(i*4):(i*4+4)]\n",
        "            char_index = sum(bit * (2**j) for j, bit in enumerate(bit_group)) % len(all_chars)\n",
        "            password.append(all_chars[char_index])\n",
        "\n",
        "\n",
        "        for i in range(len(password)):\n",
        "            j_bits = quantum_bits[(i*2):(i*2+2)] if (i*2+2) < len(quantum_bits) else [0, 1]\n",
        "            j = sum(bit * (2**k) for k, bit in enumerate(j_bits)) % len(password)\n",
        "            password[i], password[j] = password[j], password[i]\n",
        "\n",
        "        return ''.join(password)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Quantum generation failed: {e}, using secure fallback\")\n",
        "\n",
        "        import secrets\n",
        "        chars = string.ascii_letters + string.digits\n",
        "        if include_special:\n",
        "            chars += \"!@#$%&*\"\n",
        "\n",
        "        password = []\n",
        "        if length >= 4:\n",
        "            password.append(secrets.choice(string.ascii_lowercase))\n",
        "            password.append(secrets.choice(string.ascii_uppercase))\n",
        "            password.append(secrets.choice(string.digits))\n",
        "            if include_special:\n",
        "                password.append(secrets.choice(\"!@#$%&*\"))\n",
        "\n",
        "        for _ in range(len(password), length):\n",
        "            password.append(secrets.choice(chars))\n",
        "\n",
        "\n",
        "        for i in range(len(password)):\n",
        "            j = secrets.randbelow(len(password))\n",
        "            password[i], password[j] = password[j], password[i]\n",
        "\n",
        "        return ''.join(password)\n",
        "\n",
        "@app.get(\"/verify-token\")\n",
        "async def test_token_verification(username: str = Depends(verify_token)):\n",
        "    return {\"message\": f\"Token is valid for user: {username}\", \"success\": True}\n",
        "\n",
        "\n",
        "@app.get(\"/test-no-auth\")\n",
        "async def test_no_auth():\n",
        "    return {\"message\": \"This endpoint works without authentication\", \"success\": True}\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    return {\"message\": \"Quantum Fraud Detection API v2.0 is running\"}\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health_check():\n",
        "    return {\"status\": \"healthy\", \"quantum_device\": str(dev)}\n",
        "\n",
        "@app.post(\"/generate-quantum-password\")\n",
        "async def generate_password(request: PasswordGenerateRequest):\n",
        "    try:\n",
        "        if request.length < 6 or request.length > 50:\n",
        "            raise HTTPException(status_code=400, detail=\"Password length must be between 6 and 50 characters\")\n",
        "\n",
        "        password = generate_quantum_password(\n",
        "            length=request.length,\n",
        "            include_special=request.include_special\n",
        "        )\n",
        "\n",
        "\n",
        "        strength_score = 0\n",
        "        if len(password) >= 8:\n",
        "            strength_score += 1\n",
        "        if any(c.isupper() for c in password):\n",
        "            strength_score += 1\n",
        "        if any(c.islower() for c in password):\n",
        "            strength_score += 1\n",
        "        if any(c.isdigit() for c in password):\n",
        "            strength_score += 1\n",
        "        if any(c in \"!@#$%&*\" for c in password):\n",
        "            strength_score += 1\n",
        "\n",
        "        strength_levels = [\"Very Weak\", \"Weak\", \"Fair\", \"Good\", \"Strong\", \"Very Strong\"]\n",
        "        strength = strength_levels[min(strength_score, 5)]\n",
        "\n",
        "        return {\n",
        "            \"success\": True,\n",
        "            \"password\": password,\n",
        "            \"strength\": strength,\n",
        "            \"length\": len(password),\n",
        "            \"has_uppercase\": any(c.isupper() for c in password),\n",
        "            \"has_lowercase\": any(c.islower() for c in password),\n",
        "            \"has_numbers\": any(c.isdigit() for c in password),\n",
        "            \"has_special\": any(c in \"!@#$%&*\" for c in password)\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"success\": False,\n",
        "            \"message\": f\"Password generation failed: {str(e)}\"\n",
        "        }\n",
        "\n",
        "@app.post(\"/upload-csv\")\n",
        "async def upload_csv(file: UploadFile = File(...)):\n",
        "    try:\n",
        "        content = await file.read()\n",
        "        df = pd.read_csv(io.StringIO(content.decode('utf-8')))\n",
        "\n",
        "        required_cols = [\"amount\", \"time\", \"device\", \"merchant_risk\", \"label\",\n",
        "                        \"merchant_category\", \"transaction_type\", \"cardholder_age\"]\n",
        "        missing_cols = [col for col in required_cols if col.lower() not in [c.lower() for c in df.columns]]\n",
        "\n",
        "        if missing_cols:\n",
        "            raise HTTPException(status_code=400, detail=f\"Missing columns: {missing_cols}\")\n",
        "\n",
        "        return {\n",
        "            \"success\": True,\n",
        "            \"message\": f\"CSV uploaded successfully with {len(df)} rows\",\n",
        "            \"preview\": df.head().to_dict('records'),\n",
        "            \"columns\": df.columns.tolist(),\n",
        "            \"shape\": df.shape\n",
        "        }\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=400, detail=str(e))\n",
        "\n",
        "\n",
        "@app.post(\"/upload-creditcard-csv\")\n",
        "async def upload_creditcard_csv(file: UploadFile = File(...)):\n",
        "    global creditcard_data\n",
        "    try:\n",
        "        content = await file.read()\n",
        "        df = pd.read_csv(io.StringIO(content.decode('utf-8')))\n",
        "\n",
        "\n",
        "        required_cols = ['Time', 'Amount', 'Class'] + [f'V{i}' for i in range(1, 29)]\n",
        "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "\n",
        "        if missing_cols:\n",
        "            raise HTTPException(status_code=400, detail=f\"Missing columns for credit card dataset: {missing_cols}\")\n",
        "\n",
        "        creditcard_data = df\n",
        "\n",
        "        return {\n",
        "            \"success\": True,\n",
        "            \"message\": f\"Credit card CSV uploaded successfully with {len(df)} rows\",\n",
        "            \"fraud_count\": int(df['Class'].sum()),\n",
        "            \"legitimate_count\": int((df['Class'] == 0).sum()),\n",
        "            \"fraud_percentage\": float(df['Class'].mean() * 100),\n",
        "            \"shape\": df.shape\n",
        "        }\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=400, detail=str(e))\n",
        "\n",
        "DEFAULT_CREDITCARD_CSV = \"./creditcard.csv\"\n",
        "\n",
        "@app.post(\"/train-creditcard-quantum\", response_model=TrainingResponse)\n",
        "async def train_creditcard_quantum_model(config: TrainingConfig):\n",
        "    global creditcard_quantum_model, creditcard_scaler_quantum, creditcard_data\n",
        "\n",
        "\n",
        "    if creditcard_data is None:\n",
        "        if os.path.exists(DEFAULT_CREDITCARD_CSV):\n",
        "            creditcard_data = pd.read_csv(DEFAULT_CREDITCARD_CSV)\n",
        "        else:\n",
        "            raise HTTPException(status_code=400, detail=\"Credit card dataset not uploaded, and default CSV not found.\")\n",
        "\n",
        "    try:\n",
        "\n",
        "        feature_cols = ['Time', 'Amount', 'V1', 'V2']\n",
        "        X = creditcard_data[feature_cols].values\n",
        "        y = creditcard_data['Class'].values\n",
        "\n",
        "\n",
        "        creditcard_scaler_quantum = StandardScaler()\n",
        "        X_scaled = creditcard_scaler_quantum.fit_transform(X)\n",
        "\n",
        "\n",
        "        n_samples = min(5000, len(X_scaled))\n",
        "        indices = np.random.choice(len(X_scaled), n_samples, replace=False)\n",
        "        X_subset = X_scaled[indices]\n",
        "        y_subset = y[indices]\n",
        "\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_subset, y_subset, test_size=0.2, random_state=config.seed, stratify=y_subset\n",
        "        )\n",
        "\n",
        "\n",
        "        qnp.random.seed(config.seed)\n",
        "        W = qnp.array(0.01 * qnp.random.randn(2*n_qubits), requires_grad=True)\n",
        "\n",
        "\n",
        "        opt = qml.GradientDescentOptimizer(stepsize=config.stepsize)\n",
        "        X_train_q = qnp.array(X_train, requires_grad=False)\n",
        "        y_train_q = qnp.array(y_train, requires_grad=False)\n",
        "\n",
        "        for epoch in range(config.epochs):\n",
        "            idx = np.random.choice(len(X_train), min(config.batch_size, len(X_train)), replace=False)\n",
        "            batchX = X_train_q[idx]\n",
        "            batchY = y_train_q[idx]\n",
        "            W = opt.step(lambda w: bce_loss(w, batchX, batchY), W)\n",
        "\n",
        "\n",
        "        probs = predict_batch(W, qnp.array(X_test))\n",
        "        preds = (probs > 0.5).astype(int)\n",
        "        auc = roc_auc_score(y_test, np.asarray(probs))\n",
        "        f1 = f1_score(y_test, np.asarray(preds))\n",
        "        precision, recall, f1s, support = precision_recall_fscore_support(y_test, preds, zero_division=0)\n",
        "        class_report = classification_report(y_test, preds, output_dict=True)\n",
        "\n",
        "\n",
        "        creditcard_quantum_model = W\n",
        "\n",
        "        metrics = {\n",
        "            \"auc\": float(auc),\n",
        "            \"f1\": float(f1),\n",
        "            \"test_accuracy\": float(np.mean(preds == y_test)),\n",
        "            \"precision\": precision.tolist(),\n",
        "            \"recall\": recall.tolist(),\n",
        "            \"f1_per_class\": f1s.tolist(),\n",
        "            \"support\": support.tolist(),\n",
        "            \"classification_report\": class_report,\n",
        "            \"training_samples\": n_samples\n",
        "        }\n",
        "\n",
        "        return TrainingResponse(\n",
        "            success=True,\n",
        "            message=\"Credit card quantum model trained successfully\",\n",
        "            metrics=metrics\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        return TrainingResponse(\n",
        "            success=False,\n",
        "            message=f\"Training failed: {str(e)}\",\n",
        "            metrics={}\n",
        "        )\n",
        "\n",
        "\n",
        "@app.post(\"/train-creditcard-classical\", response_model=TrainingResponse)\n",
        "async def train_creditcard_classical_models():\n",
        "    global creditcard_classical_models, creditcard_scaler_classical, creditcard_data\n",
        "\n",
        "    if creditcard_data is None:\n",
        "        raise HTTPException(status_code=400, detail=\"Credit card dataset not uploaded. Please upload creditcard.csv first.\")\n",
        "\n",
        "    try:\n",
        "\n",
        "        feature_cols = ['Time', 'Amount'] + [f'V{i}' for i in range(1, 29)]\n",
        "        X = creditcard_data[feature_cols].values\n",
        "        y = creditcard_data['Class'].values\n",
        "\n",
        "\n",
        "        creditcard_scaler_classical = StandardScaler()\n",
        "        X_scaled = creditcard_scaler_classical.fit_transform(X)\n",
        "\n",
        "\n",
        "        n_samples = min(50000, len(X_scaled))\n",
        "        indices = np.random.choice(len(X_scaled), n_samples, replace=False,\n",
        "                                  p=None if len(X_scaled) < 100000 else None)\n",
        "        X_subset = X_scaled[indices]\n",
        "        y_subset = y[indices]\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_subset, y_subset, test_size=0.2, random_state=42, stratify=y_subset\n",
        "        )\n",
        "\n",
        "\n",
        "        rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
        "        lr_model = LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000)\n",
        "\n",
        "        rf_model.fit(X_train, y_train)\n",
        "        lr_model.fit(X_train, y_train)\n",
        "\n",
        "        # Evaluate\n",
        "        rf_preds = rf_model.predict(X_test)\n",
        "        lr_preds = lr_model.predict(X_test)\n",
        "\n",
        "        rf_probs = rf_model.predict_proba(X_test)[:, 1]\n",
        "        lr_probs = lr_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        rf_auc = roc_auc_score(y_test, rf_probs)\n",
        "        lr_auc = roc_auc_score(y_test, lr_probs)\n",
        "\n",
        "        rf_f1 = f1_score(y_test, rf_preds, zero_division=0)\n",
        "        lr_f1 = f1_score(y_test, lr_preds, zero_division=0)\n",
        "\n",
        "        rf_accuracy = np.mean(rf_preds == y_test)\n",
        "        lr_accuracy = np.mean(lr_preds == y_test)\n",
        "\n",
        "        rf_report = classification_report(y_test, rf_preds, output_dict=True, zero_division=0)\n",
        "        lr_report = classification_report(y_test, lr_preds, output_dict=True, zero_division=0)\n",
        "\n",
        "\n",
        "        creditcard_classical_models = {\n",
        "            \"random_forest\": rf_model,\n",
        "            \"logistic_regression\": lr_model\n",
        "        }\n",
        "\n",
        "        metrics = {\n",
        "            \"random_forest_auc\": float(rf_auc),\n",
        "            \"logistic_regression_auc\": float(lr_auc),\n",
        "            \"random_forest_f1\": float(rf_f1),\n",
        "            \"logistic_regression_f1\": float(lr_f1),\n",
        "            \"random_forest_accuracy\": float(rf_accuracy),\n",
        "            \"logistic_regression_accuracy\": float(lr_accuracy),\n",
        "            \"random_forest_report\": rf_report,\n",
        "            \"logistic_regression_report\": lr_report,\n",
        "            \"training_samples\": n_samples\n",
        "        }\n",
        "\n",
        "        return TrainingResponse(\n",
        "            success=True,\n",
        "            message=\"Credit card classical models trained successfully\",\n",
        "            metrics=metrics\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        return TrainingResponse(\n",
        "            success=False,\n",
        "            message=f\"Training failed: {str(e)}\",\n",
        "            metrics={}\n",
        "        )\n",
        "\n",
        "@app.post(\"/predict-creditcard\", response_model=CreditCardPredictionResponse)\n",
        "async def predict_creditcard_transaction(transaction: CreditCardInput):\n",
        "    global creditcard_quantum_model, creditcard_scaler_quantum\n",
        "    global creditcard_classical_models, creditcard_scaler_classical\n",
        "\n",
        "    if creditcard_quantum_model is None and not creditcard_classical_models:\n",
        "        raise HTTPException(status_code=400, detail=\"No credit card models trained. Please train at least one model first.\")\n",
        "\n",
        "    try:\n",
        "        results = {}\n",
        "        predictions = []\n",
        "\n",
        "\n",
        "        if creditcard_quantum_model is not None and creditcard_scaler_quantum is not None:\n",
        "            try:\n",
        "                x_quantum = np.array([\n",
        "                    transaction.Time,\n",
        "                    transaction.Amount,\n",
        "                    transaction.V1,\n",
        "                    transaction.V2\n",
        "                ])\n",
        "                x_quantum_scaled = creditcard_scaler_quantum.transform([x_quantum])\n",
        "                quantum_prob = float(predict_batch(creditcard_quantum_model, qnp.array(x_quantum_scaled))[0])\n",
        "                results[\"quantum_prediction\"] = quantum_prob\n",
        "                predictions.append(quantum_prob)\n",
        "            except Exception as e:\n",
        "                results[\"quantum_error\"] = str(e)\n",
        "\n",
        "\n",
        "        if creditcard_classical_models and creditcard_scaler_classical is not None:\n",
        "            try:\n",
        "                x_classical = np.array([\n",
        "                    transaction.Time, transaction.Amount,\n",
        "                    transaction.V1, transaction.V2, transaction.V3, transaction.V4, transaction.V5,\n",
        "                    transaction.V6, transaction.V7, transaction.V8, transaction.V9, transaction.V10,\n",
        "                    transaction.V11, transaction.V12, transaction.V13, transaction.V14, transaction.V15,\n",
        "                    transaction.V16, transaction.V17, transaction.V18, transaction.V19, transaction.V20,\n",
        "                    transaction.V21, transaction.V22, transaction.V23, transaction.V24, transaction.V25,\n",
        "                    transaction.V26, transaction.V27, transaction.V28\n",
        "                ])\n",
        "                x_classical_scaled = creditcard_scaler_classical.transform([x_classical])\n",
        "\n",
        "                if \"random_forest\" in creditcard_classical_models:\n",
        "                    rf_prob = float(creditcard_classical_models[\"random_forest\"].predict_proba(x_classical_scaled)[:, 1][0])\n",
        "                    results[\"classical_rf_prediction\"] = rf_prob\n",
        "                    predictions.append(rf_prob)\n",
        "\n",
        "                if \"logistic_regression\" in creditcard_classical_models:\n",
        "                    lr_prob = float(creditcard_classical_models[\"logistic_regression\"].predict_proba(x_classical_scaled)[:, 1][0])\n",
        "                    results[\"classical_lr_prediction\"] = lr_prob\n",
        "                    predictions.append(lr_prob)\n",
        "            except Exception as e:\n",
        "                results[\"classical_error\"] = str(e)\n",
        "\n",
        "\n",
        "        if \"quantum_prediction\" in results and \"classical_rf_prediction\" in results and \"classical_lr_prediction\" in results:\n",
        "            hybrid_prob = (\n",
        "                results[\"quantum_prediction\"] * 0.3 +\n",
        "                results[\"classical_rf_prediction\"] * 0.5 +\n",
        "                results[\"classical_lr_prediction\"] * 0.2\n",
        "            )\n",
        "            results[\"hybrid_prediction\"] = hybrid_prob\n",
        "        else:\n",
        "\n",
        "            if predictions:\n",
        "                results[\"hybrid_prediction\"] = float(np.mean(predictions))\n",
        "\n",
        "        if len(predictions) > 1:\n",
        "            results[\"confidence\"] = float(1.0 - np.std(predictions))\n",
        "        else:\n",
        "            results[\"confidence\"] = 1.0\n",
        "\n",
        "        results[\"is_fraud\"] = results[\"hybrid_prediction\"] > 0.5\n",
        "\n",
        "        return CreditCardPredictionResponse(**results)\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=400, detail=str(e))\n",
        "\n",
        "\n",
        "@app.post(\"/save-prediction\")\n",
        "async def save_prediction(\n",
        "    request: SavePredictionRequest,\n",
        "    username: str = Depends(verify_token)\n",
        "):\n",
        "    try:\n",
        "        print(f\"DEBUG: save_prediction called for user: {username}\")\n",
        "        print(f\"DEBUG: Request data: {request}\")\n",
        "\n",
        "        prediction_doc = {\n",
        "            \"user\": username,\n",
        "            \"prediction_type\": request.prediction_type,\n",
        "            \"input_data\": request.input_data,\n",
        "            \"result\": request.result,\n",
        "            \"timestamp\": datetime.utcnow()\n",
        "        }\n",
        "        print(f\"DEBUG: Saving document: {prediction_doc}\")\n",
        "\n",
        "        result_insert = await db.predictions.insert_one(prediction_doc)\n",
        "        print(f\"DEBUG: Insert result: {result_insert}\")\n",
        "\n",
        "        return {\n",
        "            \"success\": True,\n",
        "            \"message\": \"Prediction saved successfully\",\n",
        "            \"prediction_id\": str(result_insert.inserted_id)\n",
        "        }\n",
        "    except HTTPException:\n",
        "        print(\"DEBUG: HTTPException in save_prediction\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\"DEBUG: Exception in save_prediction: {str(e)}\")\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "\n",
        "@app.get(\"/saved-predictions\")\n",
        "async def get_saved_predictions(username: str = Depends(verify_token)):\n",
        "    try:\n",
        "        predictions = []\n",
        "        async for prediction in db.predictions.find({\"user\": username}).sort(\"timestamp\", -1):\n",
        "            prediction[\"_id\"] = str(prediction[\"_id\"])\n",
        "            predictions.append(prediction)\n",
        "\n",
        "        return {\n",
        "            \"success\": True,\n",
        "            \"predictions\": predictions,\n",
        "            \"count\": len(predictions)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "\n",
        "@app.delete(\"/saved-predictions/{prediction_id}\")\n",
        "async def delete_saved_prediction(prediction_id: str, username: str = Depends(verify_token)):\n",
        "    try:\n",
        "        result = await db.predictions.delete_one({\n",
        "            \"_id\": ObjectId(prediction_id),\n",
        "            \"user\": username\n",
        "        })\n",
        "\n",
        "        if result.deleted_count == 0:\n",
        "            raise HTTPException(status_code=404, detail=\"Prediction not found\")\n",
        "\n",
        "        return {\n",
        "            \"success\": True,\n",
        "            \"message\": \"Prediction deleted successfully\"\n",
        "        }\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "\n",
        "@app.post(\"/train-quantum\", response_model=TrainingResponse)\n",
        "async def train_quantum_model(config: TrainingConfig):\n",
        "    global quantum_model, scaler_quantum\n",
        "\n",
        "    try:\n",
        "\n",
        "        X, y = make_toy_fraud(n=1400, seed=config.seed)\n",
        "\n",
        "\n",
        "        scaler_quantum = StandardScaler()\n",
        "        X_scaled = scaler_quantum.fit_transform(X)\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_scaled, y, test_size=0.2, random_state=config.seed, stratify=y\n",
        "        )\n",
        "\n",
        "        qnp.random.seed(config.seed)\n",
        "        W = qnp.array(0.01 * qnp.random.randn(2*n_qubits), requires_grad=True)\n",
        "\n",
        "\n",
        "        opt = qml.GradientDescentOptimizer(stepsize=config.stepsize)\n",
        "        X_train_q = qnp.array(X_train, requires_grad=False)\n",
        "        y_train_q = qnp.array(y_train, requires_grad=False)\n",
        "\n",
        "        for epoch in range(config.epochs):\n",
        "            idx = np.random.choice(len(X_train), min(config.batch_size, len(X_train)), replace=False)\n",
        "            batchX = X_train_q[idx]\n",
        "            batchY = y_train_q[idx]\n",
        "            W = opt.step(lambda w: bce_loss(w, batchX, batchY), W)\n",
        "\n",
        "\n",
        "        probs = predict_batch(W, qnp.array(X_test))\n",
        "        preds = (probs > 0.5).astype(int)\n",
        "        auc = roc_auc_score(y_test, np.asarray(probs))\n",
        "        f1 = f1_score(y_test, np.asarray(preds))\n",
        "        precision, recall, f1s, support = precision_recall_fscore_support(y_test, preds, zero_division=0)\n",
        "        class_report = classification_report(y_test, preds, output_dict=True)\n",
        "\n",
        "\n",
        "        quantum_model = W\n",
        "\n",
        "        metrics = {\n",
        "            \"auc\": float(auc),\n",
        "            \"f1\": float(f1),\n",
        "            \"test_accuracy\": float(np.mean(preds == y_test)),\n",
        "            \"precision\": precision.tolist(),\n",
        "            \"recall\": recall.tolist(),\n",
        "            \"f1_per_class\": f1s.tolist(),\n",
        "            \"support\": support.tolist(),\n",
        "            \"classification_report\": class_report\n",
        "        }\n",
        "\n",
        "        return TrainingResponse(\n",
        "            success=True,\n",
        "            message=\"Quantum model trained successfully\",\n",
        "            metrics=metrics\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        return TrainingResponse(\n",
        "            success=False,\n",
        "            message=f\"Training failed: {str(e)}\",\n",
        "            metrics={}\n",
        "        )\n",
        "\n",
        "\n",
        "@app.post(\"/train-classical\", response_model=TrainingResponse)\n",
        "async def train_classical_models():\n",
        "    global classical_models, scaler_classical\n",
        "\n",
        "    try:\n",
        "        # Generate toy data\n",
        "        X, y = make_toy_fraud(n=1400, seed=42)\n",
        "        X_extra = np.zeros((len(X), 3))\n",
        "        X_all = np.hstack([X, X_extra])\n",
        "\n",
        "\n",
        "        scaler_classical = StandardScaler()\n",
        "        X_scaled = scaler_classical.fit_transform(X_all)\n",
        "\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
        "        )\n",
        "\n",
        "        rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "        lr_model = LogisticRegression(random_state=42)\n",
        "\n",
        "        rf_model.fit(X_train, y_train)\n",
        "        lr_model.fit(X_train, y_train)\n",
        "\n",
        "        rf_preds = rf_model.predict(X_test)\n",
        "        lr_preds = lr_model.predict(X_test)\n",
        "\n",
        "        rf_probs = rf_model.predict_proba(X_test)[:, 1]\n",
        "        lr_probs = lr_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "\n",
        "        rf_auc = roc_auc_score(y_test, rf_probs)\n",
        "        lr_auc = roc_auc_score(y_test, lr_probs)\n",
        "\n",
        "        rf_f1 = f1_score(y_test, rf_preds, zero_division=0)\n",
        "        lr_f1 = f1_score(y_test, lr_preds, zero_division=0)\n",
        "\n",
        "        rf_accuracy = np.mean(rf_preds == y_test)\n",
        "        lr_accuracy = np.mean(lr_preds == y_test)\n",
        "\n",
        "        rf_report = classification_report(y_test, rf_preds, output_dict=True, zero_division=0)\n",
        "        lr_report = classification_report(y_test, lr_preds, output_dict=True, zero_division=0)\n",
        "\n",
        "        classical_models = {\n",
        "            \"random_forest\": rf_model,\n",
        "            \"logistic_regression\": lr_model\n",
        "        }\n",
        "\n",
        "        metrics = {\n",
        "            \"random_forest_auc\": float(rf_auc),\n",
        "            \"logistic_regression_auc\": float(lr_auc),\n",
        "            \"random_forest_f1\": float(rf_f1),\n",
        "            \"logistic_regression_f1\": float(lr_f1),\n",
        "            \"random_forest_accuracy\": float(rf_accuracy),\n",
        "            \"logistic_regression_accuracy\": float(lr_accuracy),\n",
        "            \"random_forest_report\": rf_report,\n",
        "            \"logistic_regression_report\": lr_report\n",
        "        }\n",
        "\n",
        "        return TrainingResponse(\n",
        "            success=True,\n",
        "            message=\"Classical models trained successfully\",\n",
        "            metrics=metrics\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        return TrainingResponse(\n",
        "            success=False,\n",
        "            message=f\"Training failed: {str(e)}\",\n",
        "            metrics={}\n",
        "        )\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        return TrainingResponse(\n",
        "            success=False,\n",
        "            message=f\"Training failed: {str(e)}\",\n",
        "            metrics={}\n",
        "        )\n",
        "\n",
        "@app.post(\"/predict\", response_model=PredictionResponse)\n",
        "async def predict_transaction(transaction: TransactionInput):\n",
        "    global quantum_model, scaler_quantum, classical_models, scaler_classical\n",
        "\n",
        "    if quantum_model is None and not classical_models:\n",
        "        raise HTTPException(status_code=400, detail=\"No models trained. Train at least one model first.\")\n",
        "\n",
        "    try:\n",
        "\n",
        "        device_map = {\"Mobile\": 0.2, \"Desktop\": 0.5, \"ATM\": 0.8}\n",
        "        cat_map = {\"Electronics\": 0.0, \"Grocery\": 0.5, \"Entertainment\": 1.0}\n",
        "        type_map = {\"Online\": 0.2, \"In-Person\": 0.5, \"ATM\": 0.8}\n",
        "\n",
        "\n",
        "        x_quantum = np.array([\n",
        "            transaction.amount / 1000.0,\n",
        "            transaction.hour / 24.0,\n",
        "            device_map.get(transaction.device, 0.5),\n",
        "            transaction.merchant_risk\n",
        "        ])\n",
        "\n",
        "        x_extra = np.array([\n",
        "            cat_map.get(transaction.merchant_category, 0.5),\n",
        "            type_map.get(transaction.transaction_type, 0.5),\n",
        "            transaction.cardholder_age / 100.0\n",
        "        ])\n",
        "        x_classical = np.hstack([x_quantum, x_extra])\n",
        "\n",
        "        results = {}\n",
        "        predictions = []\n",
        "\n",
        "\n",
        "        if quantum_model is not None and scaler_quantum is not None:\n",
        "            try:\n",
        "                x_quantum_scaled = scaler_quantum.transform([x_quantum])\n",
        "                quantum_prob = float(predict_batch(quantum_model, qnp.array(x_quantum_scaled))[0])\n",
        "                results[\"quantum_prediction\"] = quantum_prob\n",
        "                predictions.append(quantum_prob)\n",
        "            except Exception as e:\n",
        "                results[\"quantum_error\"] = str(e)\n",
        "\n",
        "\n",
        "        if classical_models and scaler_classical is not None:\n",
        "            try:\n",
        "                x_classical_scaled = scaler_classical.transform([x_classical])\n",
        "                if \"random_forest\" in classical_models:\n",
        "                    rf_prob = float(classical_models[\"random_forest\"].predict_proba(x_classical_scaled)[:, 1][0])\n",
        "                    results[\"classical_rf_prediction\"] = rf_prob\n",
        "                    predictions.append(rf_prob)\n",
        "\n",
        "                if \"logistic_regression\" in classical_models:\n",
        "                    lr_prob = float(classical_models[\"logistic_regression\"].predict_proba(x_classical_scaled)[:, 1][0])\n",
        "                    results[\"classical_lr_prediction\"] = lr_prob\n",
        "                    predictions.append(lr_prob)\n",
        "            except Exception as e:\n",
        "                results[\"classical_error\"] = str(e)\n",
        "\n",
        "        if predictions:\n",
        "            results[\"hybrid_prediction\"] = float(np.mean(predictions))\n",
        "        else:\n",
        "            results[\"hybrid_prediction\"] = 0.0\n",
        "\n",
        "        results[\"is_fraud\"] = results[\"hybrid_prediction\"] > 0.5\n",
        "\n",
        "        return PredictionResponse(**results)\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=400, detail=str(e))\n",
        "\n",
        "@app.get(\"/analytics\", response_model=AnalyticsResponse)\n",
        "async def get_analytics():\n",
        "    global quantum_model, classical_models, scaler_quantum, scaler_classical\n",
        "\n",
        "    if quantum_model is None or not classical_models:\n",
        "        raise HTTPException(status_code=400, detail=\"Models not trained. Please train models first.\")\n",
        "\n",
        "    try:\n",
        "\n",
        "        X, y = make_toy_fraud(n=500, seed=999)\n",
        "        X_extra = np.zeros((len(X), 3))\n",
        "        X_all = np.hstack([X, X_extra])\n",
        "\n",
        "        X_quantum_scaled = scaler_quantum.transform(X)\n",
        "        X_classical_scaled = scaler_classical.transform(X_all)\n",
        "\n",
        "        quantum_probs = np.array([float(p) for p in predict_batch(quantum_model, qnp.array(X_quantum_scaled))])\n",
        "        rf_probs = classical_models[\"random_forest\"].predict_proba(X_classical_scaled)[:, 1]\n",
        "        lr_probs = classical_models[\"logistic_regression\"].predict_proba(X_classical_scaled)[:, 1]\n",
        "\n",
        "        quantum_auc = roc_auc_score(y, quantum_probs)\n",
        "        rf_auc = roc_auc_score(y, rf_probs)\n",
        "        lr_auc = roc_auc_score(y, lr_probs)\n",
        "\n",
        "\n",
        "        feature_names = [\"Amount\", \"Time\", \"Device\", \"Merchant Risk\"]\n",
        "        rf_importance = classical_models[\"random_forest\"].feature_importances_[:4]  # First 4 features\n",
        "\n",
        "        quantum_preds = (quantum_probs > 0.5).astype(int)\n",
        "        cm = confusion_matrix(y, quantum_preds).tolist()\n",
        "\n",
        "        return AnalyticsResponse(\n",
        "            model_performance={\n",
        "                \"quantum_auc\": float(quantum_auc),\n",
        "                \"random_forest_auc\": float(rf_auc),\n",
        "                \"logistic_regression_auc\": float(lr_auc)\n",
        "            },\n",
        "            feature_importance={\n",
        "                feature_names[i]: float(rf_importance[i]) for i in range(len(feature_names))\n",
        "            },\n",
        "            confusion_matrix=cm\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=400, detail=str(e))\n",
        "\n",
        "\n",
        "@app.get(\"/creditcard-analytics\", response_model=AnalyticsResponse)\n",
        "async def get_creditcard_analytics(username: str = Depends(verify_token)):\n",
        "    global creditcard_quantum_model, creditcard_classical_models, creditcard_scaler_quantum, creditcard_scaler_classical, creditcard_data\n",
        "\n",
        "    if creditcard_quantum_model is None or not creditcard_classical_models or creditcard_data is None:\n",
        "        raise HTTPException(status_code=400, detail=\"Credit card models not trained or data not loaded.\")\n",
        "\n",
        "    try:\n",
        "\n",
        "        n_samples = min(1000, len(creditcard_data))\n",
        "        sample_data = creditcard_data.sample(n=n_samples, random_state=42)\n",
        "\n",
        "        feature_cols_quantum = ['Time', 'Amount', 'V1', 'V2']\n",
        "        feature_cols_classical = ['Time', 'Amount'] + [f'V{i}' for i in range(1, 29)]\n",
        "\n",
        "        X_quantum = sample_data[feature_cols_quantum].values\n",
        "        X_classical = sample_data[feature_cols_classical].values\n",
        "        y = sample_data['Class'].values\n",
        "\n",
        "        X_quantum_scaled = creditcard_scaler_quantum.transform(X_quantum)\n",
        "        X_classical_scaled = creditcard_scaler_classical.transform(X_classical)\n",
        "\n",
        "        quantum_probs = np.array([float(p) for p in predict_batch(creditcard_quantum_model, qnp.array(X_quantum_scaled))])\n",
        "        rf_probs = creditcard_classical_models[\"random_forest\"].predict_proba(X_classical_scaled)[:, 1]\n",
        "        lr_probs = creditcard_classical_models[\"logistic_regression\"].predict_proba(X_classical_scaled)[:, 1]\n",
        "\n",
        "\n",
        "        quantum_auc = roc_auc_score(y, quantum_probs)\n",
        "        rf_auc = roc_auc_score(y, rf_probs)\n",
        "        lr_auc = roc_auc_score(y, lr_probs)\n",
        "\n",
        "        feature_names = ['Time', 'Amount'] + [f'V{i}' for i in range(1, 29)]\n",
        "        rf_importance = creditcard_classical_models[\"random_forest\"].feature_importances_\n",
        "\n",
        "        top_indices = np.argsort(rf_importance)[-10:]\n",
        "        top_features = {feature_names[i]: float(rf_importance[i]) for i in top_indices}\n",
        "\n",
        "\n",
        "        quantum_preds = (quantum_probs > 0.5).astype(int)\n",
        "        cm = confusion_matrix(y, quantum_preds).tolist()\n",
        "\n",
        "        return AnalyticsResponse(\n",
        "            model_performance={\n",
        "                \"quantum_auc\": float(quantum_auc),\n",
        "                \"random_forest_auc\": float(rf_auc),\n",
        "                \"logistic_regression_auc\": float(lr_auc)\n",
        "            },\n",
        "            feature_importance=top_features,\n",
        "            confusion_matrix=cm\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=400, detail=str(e))\n",
        "\n",
        "@app.post(\"/simulate-transactions\")\n",
        "async def simulate_random_transactions(count: int = 10):\n",
        "    global quantum_model, classical_models, scaler_quantum, scaler_classical\n",
        "\n",
        "    if quantum_model is None or not classical_models:\n",
        "        raise HTTPException(status_code=400, detail=\"Models not trained. Please train models first.\")\n",
        "\n",
        "    try:\n",
        "        results = []\n",
        "        device_map = {\"Mobile\": 0.2, \"Desktop\": 0.5, \"ATM\": 0.8}\n",
        "        cat_map = {\"Electronics\": 0.0, \"Grocery\": 0.5, \"Entertainment\": 1.0}\n",
        "        type_map = {\"Online\": 0.2, \"In-Person\": 0.5, \"ATM\": 0.8}\n",
        "\n",
        "        for i in range(count):\n",
        "\n",
        "            amount = np.random.randint(10, 2000)\n",
        "            hour = np.random.randint(0, 24)\n",
        "            device = np.random.choice([\"Mobile\", \"Desktop\", \"ATM\"])\n",
        "            merchant_risk = np.random.rand()\n",
        "            category = np.random.choice([\"Electronics\", \"Grocery\", \"Entertainment\"])\n",
        "            trans_type = np.random.choice([\"Online\", \"In-Person\", \"ATM\"])\n",
        "            age = np.random.randint(18, 80)\n",
        "\n",
        "\n",
        "            x_quantum = np.array([\n",
        "                amount / 1000.0, hour / 24.0,\n",
        "                device_map[device], merchant_risk\n",
        "            ])\n",
        "            x_extra = np.array([\n",
        "                cat_map[category], type_map[trans_type], age / 100.0\n",
        "            ])\n",
        "            x_classical = np.hstack([x_quantum, x_extra])\n",
        "\n",
        "\n",
        "            x_quantum_scaled = scaler_quantum.transform([x_quantum])\n",
        "            x_classical_scaled = scaler_classical.transform([x_classical])\n",
        "\n",
        "            quantum_prob = float(predict_batch(quantum_model, qnp.array(x_quantum_scaled))[0])\n",
        "            rf_prob = float(classical_models[\"random_forest\"].predict_proba(x_classical_scaled)[:, 1][0])\n",
        "            hybrid_prob = (quantum_prob + rf_prob) / 2\n",
        "\n",
        "            results.append({\n",
        "                \"id\": i + 1,\n",
        "                \"amount\": amount,\n",
        "                \"hour\": hour,\n",
        "                \"device\": device,\n",
        "                \"merchant_risk\": round(merchant_risk, 3),\n",
        "                \"category\": category,\n",
        "                \"transaction_type\": trans_type,\n",
        "                \"age\": age,\n",
        "                \"quantum_prediction\": round(quantum_prob, 3),\n",
        "                \"rf_prediction\": round(rf_prob, 3),\n",
        "                \"hybrid_prediction\": round(hybrid_prob, 3),\n",
        "                \"is_fraud\": hybrid_prob > 0.5\n",
        "            })\n",
        "\n",
        "\n",
        "            await asyncio.sleep(0.1)\n",
        "\n",
        "        return {\"transactions\": results}\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=400, detail=str(e))\n",
        "\n",
        "@app.get(\"/model-status\")\n",
        "async def get_model_status():\n",
        "    return {\n",
        "        \"quantum_model_trained\": quantum_model is not None,\n",
        "        \"classical_models_trained\": bool(classical_models),\n",
        "        \"creditcard_quantum_model_trained\": creditcard_quantum_model is not None,\n",
        "        \"creditcard_classical_models_trained\": bool(creditcard_classical_models),\n",
        "        \"creditcard_data_loaded\": creditcard_data is not None,\n",
        "        \"available_models\": list(classical_models.keys()) if classical_models else [],\n",
        "        \"available_creditcard_models\": list(creditcard_classical_models.keys()) if creditcard_classical_models else []\n",
        "    }\n",
        "\n",
        "\n",
        "@app.get(\"/user-stats\")\n",
        "async def get_user_stats(username: str = Depends(verify_token)):\n",
        "    try:\n",
        "\n",
        "        pipeline = [\n",
        "            {\"$match\": {\"user\": username}},\n",
        "            {\"$group\": {\n",
        "                \"_id\": \"$prediction_type\",\n",
        "                \"count\": {\"$sum\": 1}\n",
        "            }}\n",
        "        ]\n",
        "\n",
        "        prediction_counts = {}\n",
        "        async for result in db.predictions.aggregate(pipeline):\n",
        "            prediction_counts[result[\"_id\"]] = result[\"count\"]\n",
        "\n",
        "\n",
        "        recent_predictions = []\n",
        "        async for prediction in db.predictions.find({\"user\": username}).sort(\"timestamp\", -1).limit(5):\n",
        "            prediction[\"_id\"] = str(prediction[\"_id\"])\n",
        "            recent_predictions.append(prediction)\n",
        "\n",
        "\n",
        "        total_predictions = await db.predictions.count_documents({\"user\": username})\n",
        "\n",
        "        return {\n",
        "            \"username\": username,\n",
        "            \"total_predictions\": total_predictions,\n",
        "            \"prediction_counts\": prediction_counts,\n",
        "            \"recent_predictions\": recent_predictions\n",
        "        }\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "\n",
        "@app.post(\"/predict-creditcard-batch\")\n",
        "async def predict_creditcard_batch(\n",
        "    transactions: List[CreditCardInput],\n",
        "    username: str = Depends(verify_token)\n",
        "):\n",
        "    global creditcard_quantum_model, creditcard_scaler_quantum, creditcard_classical_models, creditcard_scaler_classical\n",
        "\n",
        "    if creditcard_quantum_model is None or not creditcard_classical_models:\n",
        "        raise HTTPException(status_code=400, detail=\"Credit card models not trained. Please train models first.\")\n",
        "\n",
        "    try:\n",
        "        results = []\n",
        "\n",
        "        for i, transaction in enumerate(transactions):\n",
        "\n",
        "            x_quantum = np.array([\n",
        "                transaction.Time,\n",
        "                transaction.Amount,\n",
        "                transaction.V1,\n",
        "                transaction.V2\n",
        "            ])\n",
        "\n",
        "            x_classical = np.array([\n",
        "                transaction.Time, transaction.Amount,\n",
        "                transaction.V1, transaction.V2, transaction.V3, transaction.V4, transaction.V5,\n",
        "                transaction.V6, transaction.V7, transaction.V8, transaction.V9, transaction.V10,\n",
        "                transaction.V11, transaction.V12, transaction.V13, transaction.V14, transaction.V15,\n",
        "                transaction.V16, transaction.V17, transaction.V18, transaction.V19, transaction.V20,\n",
        "                transaction.V21, transaction.V22, transaction.V23, transaction.V24, transaction.V25,\n",
        "                transaction.V26, transaction.V27, transaction.V28\n",
        "            ])\n",
        "\n",
        "\n",
        "            x_quantum_scaled = creditcard_scaler_quantum.transform([x_quantum])\n",
        "            x_classical_scaled = creditcard_scaler_classical.transform([x_classical])\n",
        "\n",
        "\n",
        "            quantum_prob = float(predict_batch(creditcard_quantum_model, qnp.array(x_quantum_scaled))[0])\n",
        "            rf_prob = float(creditcard_classical_models[\"random_forest\"].predict_proba(x_classical_scaled)[:, 1][0])\n",
        "            lr_prob = float(creditcard_classical_models[\"logistic_regression\"].predict_proba(x_classical_scaled)[:, 1][0])\n",
        "\n",
        "            hybrid_prob = (quantum_prob * 0.3 + rf_prob * 0.5 + lr_prob * 0.2)\n",
        "\n",
        "            predictions = [quantum_prob, rf_prob, lr_prob]\n",
        "            confidence = 1.0 - np.std(predictions)\n",
        "\n",
        "            results.append({\n",
        "                \"transaction_id\": i + 1,\n",
        "                \"quantum_prediction\": round(quantum_prob, 4),\n",
        "                \"rf_prediction\": round(rf_prob, 4),\n",
        "                \"lr_prediction\": round(lr_prob, 4),\n",
        "                \"hybrid_prediction\": round(hybrid_prob, 4),\n",
        "                \"is_fraud\": hybrid_prob > 0.5,\n",
        "                \"confidence\": round(float(confidence), 4),\n",
        "                \"amount\": transaction.Amount\n",
        "            })\n",
        "\n",
        "        fraud_count = sum(1 for r in results if r[\"is_fraud\"])\n",
        "        avg_confidence = np.mean([r[\"confidence\"] for r in results])\n",
        "        total_amount = sum(r[\"amount\"] for r in results)\n",
        "        fraud_amount = sum(r[\"amount\"] for r in results if r[\"is_fraud\"])\n",
        "\n",
        "        return {\n",
        "            \"results\": results,\n",
        "            \"summary\": {\n",
        "                \"total_transactions\": len(transactions),\n",
        "                \"fraud_detected\": fraud_count,\n",
        "                \"fraud_percentage\": round((fraud_count / len(transactions)) * 100, 2),\n",
        "                \"average_confidence\": round(avg_confidence, 4),\n",
        "                \"total_amount\": round(total_amount, 2),\n",
        "                \"potential_fraud_amount\": round(fraud_amount, 2)\n",
        "            }\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=400, detail=str(e))\n",
        "\n",
        "@app.get(\"/export-predictions\")\n",
        "async def export_predictions(username: str = Depends(verify_token)):\n",
        "    try:\n",
        "        predictions = []\n",
        "        async for prediction in db.predictions.find({\"user\": username}).sort(\"timestamp\", -1):\n",
        "            prediction[\"_id\"] = str(prediction[\"_id\"])\n",
        "            predictions.append(prediction)\n",
        "\n",
        "\n",
        "        if predictions:\n",
        "            df = pd.DataFrame(predictions)\n",
        "            csv_buffer = io.StringIO()\n",
        "            df.to_csv(csv_buffer, index=False)\n",
        "            csv_content = csv_buffer.getvalue()\n",
        "\n",
        "            return {\n",
        "                \"success\": True,\n",
        "                \"csv_data\": csv_content,\n",
        "                \"count\": len(predictions)\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                \"success\": True,\n",
        "                \"csv_data\": \"\",\n",
        "                \"count\": 0,\n",
        "                \"message\": \"No predictions found\"\n",
        "            }\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "# =========================\n",
        "# Start FastAPI + ngrok in Colab\n",
        "# =========================\n",
        "if __name__ == \"__main__\":\n",
        "    nest_asyncio.apply()\n",
        "\n",
        "    # Start ngrok tunnel\n",
        "    public_url = ngrok.connect(8000)\n",
        "    print(\"🚀 FastAPI is live at:\", public_url)\n",
        "\n",
        "    # Allow CORS for ngrok URL\n",
        "    app.add_middleware(\n",
        "        CORSMiddleware,\n",
        "        allow_origins=[str(public_url), \"http://localhost:3000\", \"http://localhost:5173\"],\n",
        "        allow_credentials=True,\n",
        "        allow_methods=[\"*\"],\n",
        "        allow_headers=[\"*\"],\n",
        "    )\n",
        "\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1092aFH3I5yx"
      },
      "outputs": [],
      "source": [
        "!python app.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhPeh7PxORT0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3pFr52kWveMXhHzJKXGvS",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}